# -*- coding: utf-8 -*-
"""SST2 full_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QTakmgzu2VAG0TAYXG0NKyyOy4e0q8Jl
"""

!pip install transformers accelerate bitsandbytes

from google.colab import drive
drive.mount('/content/drive')

# Collect prompts from WiC

import json
# add local file when running
json_file_path = '/content/drive/My Drive/sst2_0to1000.json'

# Read the JSON file
with open(json_file_path, 'r') as json_file:
    data = json.load(json_file)

print(data)

# load meta-llama/Llama-2-13b-chat-hf and personal access token for llama2 weights. Baseline Chat based model to test WiC data

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model_name = "meta-llama/Llama-2-13b-chat-hf" # meta-llama/Llama-2-7b-chat-hf or meta-llama/Llama-2-13b-chat-hf
prompt = "Tell me about gravity"
access_token = "" # My personal access token to llama2 weights. Please don't misuse



model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, use_auth_token=access_token)
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)
model_inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")

'''
  Inputs: prompt (String)

  Output: tokenized output from the model.

  Description:
  genAndPrint takes in designed prompts and runs it through the model member function and returns the chat-based return the model gives back.
'''
def genAndPrint(prompt):
  model_inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")
  output = model.generate(**model_inputs, max_new_tokens=150,  do_sample=True)
  # print(tokenizer.decode(output[0], skip_special_tokens=True))
  return tokenizer.decode(output[0], skip_special_tokens=True)

'''
  Inputs: prompt (String)

  Output: tokenized output from the model.

  Description:
  genAndPrint takes in designed prompts and runs it through the model member function and returns the chat-based return the model gives back.
'''
def genAndPrintBaseline(prompt):
  model_inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")
  output = model.generate(**model_inputs, max_new_tokens=75,  do_sample=True)
  # print(tokenizer.decode(output[0], skip_special_tokens=True))
  return tokenizer.decode(output[0], skip_special_tokens=True)

num_examples_to_generate = 4
def generatepos():
  res = f"""Generate {num_examples_to_generate} diverse positive sentiment sentences with different sentence structures.
  Sentence 1:"""
  return res

def generateneg():
  res = f"""Generate {num_examples_to_generate} diverse negative sentiment sentences with different sentence structures.
  Sentence 1:"""
  return res

# s1 = 'The expanded window will give us time to catch the thieves'
# s2 = 'You have a two-hour window of clear weather to finish working on the lawn'
# target = 'window'
#t1 = generateSameSenseExamplesPrompt(s1, target)
# t1 = parse_sentences(genAndPrint(generatepos()))
t1 = genAndPrint(generatepos())
print(t1)

t2 = genAndPrint(generateneg())
print(t2)

# Splitting the text into parts based on the distinct patterns
parts = t2.split("Sentence ")
print(parts)

# Extracting sentences
negative_sentences = []
for part in parts[1:]:  # Skipping the first part, as it doesn't contain a sentence

    sentence = part.split(":", 1)[1].strip()
    negative_sentences.append(sentence)

negative_sentences

'''
  Inputs: text (String)

  Output: sentences (String[])

  Description:
  parse_sentences takes in the String text which is generated from the model, and splits the newly generated text by line in order to get different examples.
'''
def sst_parse_sentences(text):

  parts = text.split("Sentence ")
  # print(parts)

  # Extracting sentences
  sentences = []
  for part in parts[1:]:  # Skipping the first part, as it doesn't contain a sentence

      sentence = part.split(":", 1)[1].strip()
      sentences.append(sentence)

  return sentences

print(sst_parse_sentences(t2))







# import random

# def generate_and_print_sentences_SST2(s, pos, neg):
#     final_sentence = f"The task is to label whether each sentence has positive or negative sentiment. Answer 'positive' or 'negative'.
#     combinedExamples = neg + pos

#     random.shuffle(combinedExamples)
#     for item_s1 in combinedExamples:
#         if item_s1 in pos:
#             final_sentence += f"{item_s1}\nSentiment: positive\n"
#         else:
#             final_sentence += f"{item_s1}\nSentiment: negative\n"

#     final_sentence += f"Sentence: {s}.\nSentiment:"
#     return final_sentence

# print(generate_and_print_sentences_SST2("remains utterly satisfied to remain the same throughout", sst_parse_sentences(t1), sst_parse_sentences(t2)))

# f"""Determine if the word '{target}' in both sentences is used with a broadly similar meaning or if there's a significant shift in its usage. Focus on the overarching sense of the word, rather than subtle nuances.
# Sentence 1: {s1}
# Sentence 2: {s2}
# Does *{target}' have a broadly similar meaning in both sentences? Answer 'yes' for similar or 'no' for different.
# Answer:"""

'''
  Description:
  create_combined_prompt is our finalized helper function that takes in the model's generated examples and prompts from the WiC in order to set up a better total prompt for our model to respond to (with better context).
'''
import random

def create_combined_prompt_SST2(s1, examplespositive, examplesnegative):
    final_sentence = f"""Determine if each sentence has generally positive or negative sentiment. Answer 'positive' or 'negative'.
"""
    combinedexamples = examplespositive + examplesnegative
    random.shuffle(combinedexamples)



    for item_s1 in combinedexamples:
        if item_s1 in examplesnegative:
            final_sentence += f"""Sentence: {item_s1}
Answer: negative

"""
        else:
            final_sentence += f"""Sentence: {item_s1}
Answer: positive

"""
    final_sentence += f"""Sentence: {s1}
Answer:"""
    return final_sentence

print(create_combined_prompt_SST2("remains utterly satisfied to remain the same throughout", sst_parse_sentences(t1), sst_parse_sentences(t2)))

# Changed this function to expect the text to already be separated from the prompt (and thus the examples and their answers)
# Simply finds which occurs first in the lowercased answer: 'same' or 'different' and returns that word.
# If neither are present it just returns 'same'
def extraction(text):
#   sentences = []
#   lines = text.lower().tosplit('\n')

#   for line in lines:
#       parts = line.split(':', 1)
#       if len(parts) == 2 and parts[1].strip() != "" and parts[0].find("Word sense") != -1:
#           sentence = parts[1].strip()
#           sentences.append(sentence)

#   if sentences[-1]
#   return sentences[-1] ?

    text = text.lower()
    # index_same = text.find('same')
    # index_different = text.find('different')
    index_pos = text.find('pos')
    index_neg = text.find('neg')
    if index_pos == -1 and index_neg == -1:
        return 'negative' # Random guess, couldn't find the target labels in output

    # Only max 1 is invalid, so return either the other label or whichever came first if both are valid
    if index_pos == -1:
        return 'negative'
    elif index_neg == -1:
        return 'positive'
    else:
        return 'positive' if (index_pos < index_neg) else 'negative'

def matches(text, gt):
    if gt == 1:
        if 'positive' in text:
            return True
        else:
            return False
    elif gt == 0:
        if 'negative' in text:
            return True
        else:
            return False
    else:
        print('!!!!!!!!!!!!!!!!!ERROR in matches, not positive or negative. Text:', text)

from enum import Enum

# class syntax
class VERBOSITY(Enum):
    NONE = 0
    SOME = 1
    FULL = 2
LOG_LEVEL = VERBOSITY.FULL

def log(text, level = 0):
    match level:
        case 0: # This message is critical, always display
            print(text)
        case 1: # Include this message unless logging is set to "none"
            if LOG_LEVEL == VERBOSITY.FULL or LOG_LEVEL == VERBOSITY.SOME:
                print(text)
        case 2: # Include this message unless logging is set to "none"
            if LOG_LEVEL == VERBOSITY.FULL:
                print(text)

"""Compute baseline accuracy"""

def baselinePrompt(s1):
    res = f"""Determine if the following sentence has positive or negative sentiment. Answer 'positive' or 'negative'.
    Sentence: {s1}
    Answer:"""


    return res

'''
Description:
run_pipeline is the main pipeline, which integrates the helper functions to either run a baseline test, or instead generate example prompts for each WiC question and have our model answer the question given more context.
'''

# print(create_combined_prompt_SST2("remains utterly satisfied to remain the same throughout", sst_parse_sentences(t1), sst_parse_sentences(t2)))
def run_pipeline(USE_GENERATED_EXAMPLES, LOG_LEVEL = VERBOSITY.FULL):
    # LOG_LEVEL = VERBOSITY.SOME # VERBOSITY.FULL
    # LOG_LEVEL = VERBOSITY.FULL # VERBOSITY.FULL
    num_tested = 0
    num_correct = 0

    # USE_GENERATED_EXAMPLES = False # SET TRUE OR FALSE to determine use self prompting method or baseline

    if USE_GENERATED_EXAMPLES:
        print("USING GENERATED EXAMPLES FOR CLASSIFICATION")
    else:
        print("BASELINE METHOD: NO EXAMPLES GIVEN")
    print("--------------------------------------------")

    for instance in data[:500]:
        log(instance, 2)
        s1 = instance[0]

        gt = int(instance[-1])

        if not USE_GENERATED_EXAMPLES:
            prompt = baselinePrompt(s1)
            log(f'Baseline prompt: {prompt}', 2)
            prompt_and_res = genAndPrintBaseline(prompt)
        else:
            positive = sst_parse_sentences(genAndPrint(generatepos()))
            negative = sst_parse_sentences(genAndPrint(generateneg()))
            # print(f'positive: {positive}')

            prompt = create_combined_prompt_SST2(s1, positive, negative)

            log(f'Combined_prompt: {prompt}', 2)
            prompt_and_res = genAndPrint(prompt, max_tokens=25)


        # print('p+r:', prompt_and_res)
        # remove the prompt from the answer, it is always automatically included
        final_res = prompt_and_res[len(prompt)+1:] # +1 because generate adds a trailing space to prompt.

        extracted_ans = extraction(final_res)
        is_correct_string = 'Correct' if matches(extracted_ans, gt) else 'Wrong'
        log(f'Response: {final_res} - {is_correct_string}', 2)

        output_as_number = 1 if 'same' in extracted_ans else 0 # even if using yes, no as answers this still uses same different for consistency
        # log(f'Output: {extracted_ans}', 2)
        # ans = 1 if extraction(final_res) else 0
        log(f'Sentence 1: {s1}, GT: {gt}, Our Output: {output_as_number} --- {is_correct_string}', 1)

        # Increment number of items tested, and num correct if we got it right
        num_tested += 1
        if matches(extracted_ans, gt):
            num_correct += 1

        log(f'***Running Accuracy:  {num_correct} / {num_tested} : {num_correct / num_tested}', 0)



    if USE_GENERATED_EXAMPLES:
        print("Generated examples added results:")
    else:
        print("Baseline results:")
    log(f'Final Accuracy: {num_correct} / {num_tested} : {num_correct / num_tested}', 0)

instance = data[1]
s1 = instance[0]
gt = int(instance[-1])

positive = sst_parse_sentences(generatepos())
negative = sst_parse_sentences(generateneg())


prompt = create_combined_prompt_SST2(s1, positive, negative )

log(f'Combined_prompt: {prompt}', 2)
prompt_and_res = genAndPrint(prompt)
print(prompt_and_res)

# log(f'Combined_prompt: {prompt}', 2)

# prompt_and_res = genAndPrint(prompt)
print(prompt_and_res)
final_res = prompt_and_res[len(prompt)+1:] # +1 because generate adds a trailing space to prompt.

extracted_ans = extraction(final_res)
is_correct_string = 'Correct' if matches(extracted_ans, gt) else 'Wrong'
log(f'Response: {final_res} - {is_correct_string}', 2)
print(extracted_ans)

run_pipeline(True)

run_pipeline(False)

# Find ground trouth correctness stats
gt_num_same = 0
gt_num_different = 0
print('len:', len(data))
for instance in data:
    if instance[-1] == 1:
        gt_num_same += 1
    elif instance[-1] == 0:
        gt_num_different += 1
    else:
        print('GT label is not 0 or 1')

print('Ground truth same count:', gt_num_same)
print('Ground truth different count:', gt_num_different)